{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP HW 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дарья Родионова БКЛ182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "\n",
    "import pymorphy2\n",
    "m = pymorphy2.MorphAnalyzer()\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "\n",
    "from natasha import(\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "import json\n",
    "from pymystem3 import Mystem\n",
    "ms = Mystem()\n",
    "\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> В русском тексте будут проблемы с нетипичными написаниями некоторых слов (сахару, иль). Кроме того, pos-tagger может распознать обращения (милая, нежная) и имена собственные (Великолепный) как прилагательные, а также некоторые существительные как императивы (черни) и наоборот (плачь). <p>\n",
    "\n",
    "В английском тексте, скорее всего, возникнут сложности с сокращениями (I've, etc.), фразовыми глаголами (makes up), также будут сложности в разметке званий (General как прилагательное), слов с дефисами (so-called) и восклицаниями (alas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text = '''Моя любовь к тебе сейчас — слонёнок, \n",
    "родившийся в Берлине иль Париже и топающий ватными ступнями по комнатам хозяина зверинца. \n",
    "Не предлагай ему французских булок, он может съесть лишь дольку мандарина,\n",
    "кусочек сахару или конфету. Не плачь, о нежная, что в тесной клетке он сделается посмеяньем черни,\n",
    "чтоб в нос ему пускали дым сигары приказчики под хохот мидинеток. Не думай, милая, что день настанет,\n",
    "когда, взбесившись, разорвет он цепи и побежит по улицам и будет, как автобус, давить людей вопящих.\n",
    "Нет, пусть тебе приснится он под утро в парче и меди, в страусовых перьях, как тот, \n",
    "Великолепный, что когда-то нес к трепетному Риму Ганнибала.'''\n",
    "\n",
    "eng_text = '''The last time the US was invaded was when General Francisco Villa attacked Columbus, \n",
    "New Mexico in 1916, and alas Pancho is no more, having been assassinated in his black Dodge Roadster.\n",
    "Our sociocultural perception of wolves is influenced by the media, \n",
    "and millennia-old ideas about wolves being a threat to livestock, that they’re evil, etc.\n",
    "But I’ve always had a financial commitment on top of my tithe that is not legally classified as rent.\n",
    "Well I’m curious, what is this so-called accusation against six-pointer?\n",
    "We’re saving so much by paying for those in cash it more than makes up for the interest payments going to the car loan this year.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_tagged = ['Моя_PRON', 'любовь_NOUN', 'к_PREP', 'тебе_PRON', 'сейчас_ADV', 'слонёнок_NOUN',\n",
    "              'родившийся_ADJ', 'в_PREP', 'Берлине_NOUN', 'иль_CONJ', 'Париже_NOUN',\n",
    "              'и_CONJ', 'топающий_ADJ', 'ватными_ADJ', 'ступнями_NOUN',\n",
    "              'по_PREP', 'комнатам_NOUN', 'хозяина_NOUN', 'зверинца_NOUN',\n",
    "              'Не_PART', 'предлагай_VERB', 'ему_PRON', 'французских_ADJ', 'булок_NOUN',\n",
    "              'он_PRON', 'может_VERB', 'съесть_VERB', 'лишь_PART', 'дольку_NOUN', \n",
    "              'мандарина_NOUN', 'кусочек_NOUN', 'сахару_NOUN', 'или_CONJ', 'конфету_NOUN',\n",
    "              'Не_PART', 'плачь_VERB', 'о_PREP', 'нежная_NOUN', 'что_CONJ', \n",
    "              'в_PREP', 'тесной_ADJ', 'клетке_NOUN',\n",
    "              'он_PRON', 'сделается_VERB', 'посмеяньем_NOUN', 'черни_NOUN',\n",
    "              'чтоб_PREP', 'в_PREP', 'нос_NOUN', 'ему_PRON', 'пускали_VERB', 'дым_NOUN', 'сигары_NOUN',\n",
    "              'приказчики_NOUN', 'под_PREP', 'хохот_NOUN', 'мидинеток_NOUN',\n",
    "              'Не_CONJ', 'думай_VERB', 'милая_NOUN', 'что_CONJ', 'день_NOUN', 'настанет_VERB',\n",
    "              'когда_CONJ', 'взбесившись_VERB', 'разорвет_VERB', 'он_PRON', 'цепи_NOUN',\n",
    "              'и_VERB', 'побежит_VERB', 'по_PREP', 'улицам_NOUN', 'и_CONJ', 'будет_VERB',\n",
    "              'как_CONJ', 'автобус_NOUN', 'давить_VERB', 'людей_NOUN', 'вопящих_ADJ',\n",
    "              'Нет_PART', 'пусть_PART', 'тебе_PRON', 'приснится_VERB', 'он_PRON', 'под_PRON', 'утро_NOUN',\n",
    "              'в_PREP', 'парче_NOUN', 'и_CONJ', 'меди_NOUN', 'в_PREP', 'страусовых_ADJ', 'перьях_NOUN',\n",
    "              'как_CONJ', 'тот_PRON', 'Великолепный_NOUN', 'что_CONJ', \n",
    "              'когда-то_ADV', 'нес_VERB', 'к_PREP', 'трепетному_ADJ', 'Риму_NOUN', 'Ганнибала_NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rus_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tagged = ['The_ART', 'last_ADJ', 'time_NOUN', 'the_ART', 'US_NOUN', 'was_VERB', 'invaded_VERB', 'was_VERB',\n",
    "              'when_CONJ', 'General_NOUN', 'Francisco_NOUN', 'Villa_NOUN', 'attacked_VERB', 'Columbus_NOUN', \n",
    "              'New_NOUN', 'Mexico_NOUN', 'in_PREP', '1916_NUM', 'and_CONJ', 'alas_INTJ', 'Pancho_NOUN', \n",
    "              'is_VERB', 'no_PART', 'more_ADV', 'having_VERB', 'been_VERB', 'assassinated_VERB', 'in_PREP', \n",
    "              'his_PRON', 'black_ADJ', 'Dodge_NOUN', 'Roadster_NOUN', 'Our_PRON', 'sociocultural_ADJ', \n",
    "              'perception_NOUN', 'of_PREP', 'wolves_NOUN', 'is_VERB', 'influenced_VERB', 'by_PREP', 'the_ART', \n",
    "              'media_NOUN', 'and_CONJ', 'millennia-old_ADJ', 'ideas_NOUN', 'about_PREP', 'wolves_NOUN', \n",
    "              'being_VERB', 'a_ART', 'threat_NOUN', 'to_PART', 'livestock_NOUN', 'that_ADV', 'they_PRON', \n",
    "              're_VERB', 'evil_ADJ', 'etc_ART', 'But_CONJ', 'I_PRON', 've_VERB', 'always_ADV', 'had_VERB', \n",
    "              'a_ART', 'financial_ADJ', 'commitment_NOUN', 'on_PREP', 'top_NOUN', 'of_PREP', 'my_PRON', \n",
    "              'tithe_NOUN', 'that_ADV', 'is_VERB', 'not_ADV', 'legally_ADV', 'classified_VERB', 'as_ADV', 'rent_NOUN', \n",
    "              'Well_INTJ', 'I_PRON', 'm_VERB', 'curious_ADJ', 'what_CONJ', 'is_VERB', 'this_PRON', \n",
    "              'so-called_ADJ', 'accusation_NOUN', 'against_PREP', 'six-pointer_NOUN', 'We_PRON', 're_VERB', \n",
    "              'saving_VERB', 'so_ADV', 'much_ADJ', 'by_PREP', 'paying_VERB', 'for_PREP', 'those_PRON', \n",
    "              'in_PREP', 'cash_NOUN', 'it_PRON', 'more_ADV', 'than_ADV', 'makes_VERB', 'up_PART', 'for_PREP', \n",
    "              'the_ART', 'interest_NOUN', 'payments_NOUN', 'going_VERB', 'to_PART', 'the_ART', \n",
    "              'car_NOUN', 'loan_NOUN', 'this_PRON', 'year_NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ниже соотносит предписанные теги с правильной разметкой для обоих языков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_pos = {'LATN': 'NOUN', 'PROPN': 'PRON', 'S': 'NOUN',\n",
    "           'ADJF': 'ADJ', 'ADJS': 'ADJ', 'PRTF': 'ADJ', 'PRTS': 'ADJ', 'A': 'ADJ',\n",
    "           'ADVB': 'ADV', 'COMP': 'ADV', 'PRED': 'ADV',\n",
    "           'INFN': 'VERB', 'GRND': 'VERB', 'AUX': 'VERB', 'V': 'VERB',\n",
    "           'CCONJ': 'CONJ', 'SCONJ': 'CONJ', 'ADP': 'PREP', 'PR': 'PREP',\n",
    "           'NPRO': 'PRON', 'DET': 'PRON', 'ADVPRO': 'PRON', 'SPRO': 'PRON', 'APRO': 'PRON',\n",
    "           'PRCL': 'PART', 'ADP': 'PREP', 'PR': 'PREP'} \n",
    "\n",
    "def set_pos_rus(pos, dictionary=rus_pos):\n",
    "    if pos in dictionary:\n",
    "        return dictionary[pos]\n",
    "    else:\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_pos = {'NN': 'NOUN', 'NNP': 'NOUN', 'NNS': 'NOUN', 'PDT': 'PRON', 'PRP': 'PRON', 'PRP$': 'PRON', \n",
    "           'PROPN': 'NOUN', 'VBG': 'VERB', 'VBN': 'VERB', 'VBP': 'VERB', 'VBZ': 'VERB', 'AUX': 'VERB',\n",
    "           'CC': 'CONJ', 'CD': 'NUMB', 'DT': 'ART', 'IN': 'PREP', 'JJ': 'ADJ', 'JJS': 'ADJ', 'MD': 'VERB',\n",
    "           'RB': 'ADV', 'RP': 'PART', 'TO': 'PART', 'UH': 'INTJ', 'DET': 'ART',  'NUM': 'NUMB', 'WRB': 'ADV',\n",
    "           'CCONJ': 'CONJ', 'SCONJ': 'CONJ', 'VB': 'VERB', 'ADP': 'PREP',  }\n",
    "\n",
    "def set_pos_eng(pos, dictionary=eng_pos):\n",
    "    if pos in dictionary:\n",
    "        return dictionary[pos]\n",
    "    else:\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyMorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_check(predicted, gold):\n",
    "    print('Accuracy: %.4f' % accuracy_score(predicted, gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_tokens = simple_word_tokenize(rus_text)\n",
    "pymorphy_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"'.,!?:’—»«\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in rus_tokens:\n",
    "    if token in punct:\n",
    "        continue\n",
    "    gram = m.parse(token)[0]\n",
    "    pos = gram.tag.POS\n",
    "    if not pos:\n",
    "        pos = gram.tag\n",
    "\n",
    "    p = str(set_pos_rus(pos))\n",
    "    pymorphy_result.append(token + '_' + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pymorphy_result)\n",
    "# pymorphy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9126\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(pymorphy_result, rus_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "doc = Doc(rus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_result = [i.text + '_' + set_pos_rus(i.pos) for i in doc.tokens if i.text not in punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(natasha_result)\n",
    "# natasha_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8155\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(natasha_result, rus_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyStem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in rus_tokens:\n",
    "    if token in punct:\n",
    "        continue\n",
    "    text = ms.analyze(token)\n",
    "    pos = ''\n",
    "    for word in text:\n",
    "        if 'analysis' in word:\n",
    "            gram = word.get('analysis')[0]\n",
    "            pos = gram['gr']\n",
    "            pos = pos.split('=')[0]\n",
    "            pos = pos.split(',')[0]\n",
    "                    \n",
    "            p = set_pos_rus(pos)\n",
    "            mystem_result.append(token + '_' + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mystem_result)\n",
    "# mystem_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7573\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(mystem_result, rus_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, лучше всего разметил PyMorphy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_result = []\n",
    "for token in eng_tokens:\n",
    "    if token in punct:\n",
    "        continue\n",
    "    pos = ''\n",
    "    for t in nlp(token):\n",
    "        p = t.pos_\n",
    "        if pos == '':\n",
    "            pos = p\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    spacy_result.append(token + '_' + set_pos_eng(pos, eng_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_result)\n",
    "# spacy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6783\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(spacy_result, eng_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_result = [i[0] + '_' + set_pos_eng(i[1], eng_pos) for i in nltk.pos_tag(eng_tokens) if i[0] not in punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_result)\n",
    "# nltk_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7826\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(nltk_result, eng_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18 16:16:24,393 https://nlp.informatik.hu-berlin.de/resources/models/pos-fast/en-pos-ontonotes-fast-v0.5.pt not found in cache, downloading to /var/folders/b_/1gk5pbbs15b65fy6445mbvqh0000gn/T/tmpklxpqxl7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75266317/75266317 [00:19<00:00, 3870324.74B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18 16:16:44,137 copying /var/folders/b_/1gk5pbbs15b65fy6445mbvqh0000gn/T/tmpklxpqxl7 to cache at /Users/ddrodionova/.flair/models/en-pos-ontonotes-fast-v0.5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18 16:16:44,172 removing temp file /var/folders/b_/1gk5pbbs15b65fy6445mbvqh0000gn/T/tmpklxpqxl7\n",
      "2020-10-18 16:16:44,176 loading file /Users/ddrodionova/.flair/models/en-pos-ontonotes-fast-v0.5.pt\n"
     ]
    }
   ],
   "source": [
    "pos = SequenceTagger.load('pos-fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_text = re.sub(r'[^-’\\w\\s]','', eng_text)\n",
    "for s in eng_text.split('\\n'):\n",
    "    tagged_text = []\n",
    "    text = Sentence(eng_text)\n",
    "    pos.predict(text)\n",
    "    tagged_text.append(re.sub(' <', '<', text.to_tagged_string()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_result = []\n",
    "for pair in tagged_text:\n",
    "    part = pair.split(' ')\n",
    "    result = [i.split('<')[0] + '_' + set_pos_eng(re.search('<([^>]+)>', i).group(1), eng_pos) for i in part]\n",
    "    flair_result += result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flair_result)\n",
    "# flair_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7913\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(flair_result, eng_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokens = word_tokenize(eng_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая часть задания находится в файле NLP_HW2_2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
