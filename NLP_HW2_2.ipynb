{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP HW 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дарья Родионова БКЛ182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "t = WordPunctTokenizer()\n",
    "m = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/urllib/request.py\", line 1350, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1255, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1301, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1250, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1010, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 950, in send\n",
      "    self.connect()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 921, in connect\n",
      "    self.sock = self._create_connection(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py\", line 808, in create_connection\n",
      "    raise err\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py\", line 796, in create_connection\n",
      "    sock.connect(sa)\n",
      "socket.timeout: timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/fake_useragent/utils.py\", line 64, in get\n",
      "    with contextlib.closing(urlopen(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/urllib/request.py\", line 222, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/urllib/request.py\", line 525, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/urllib/request.py\", line 542, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/urllib/request.py\", line 502, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/urllib/request.py\", line 1379, in http_open\n",
      "    return self.do_open(http.client.HTTPConnection, req)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/urllib/request.py\", line 1353, in do_open\n",
      "    raise URLError(err)\n",
      "urllib.error.URLError: <urlopen error timed out>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/fake_useragent/utils.py\", line 164, in load\n",
      "    browsers_dict[browser_key] = get_browser_versions(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/fake_useragent/utils.py\", line 120, in get_browser_versions\n",
      "    html = get(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/fake_useragent/utils.py\", line 84, in get\n",
      "    raise FakeUserAgentError('Maximum amount of retries reached')\n",
      "fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached\n"
     ]
    }
   ],
   "source": [
    "session = requests.session()\n",
    "ua = UserAgent(verify_ssl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dict = []\n",
    "\n",
    "def get_movies():\n",
    "    url = f'https://www.afisha.ru/msk/schedule_cinema/vybor-afishi/'\n",
    "    response = session.get(url, headers={'User-Agent':ua.random})\n",
    "    time.sleep(random.randint(3,10))\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    for m in soup.find_all('h3', {'class':'heHLK'}):\n",
    "        movies_dict.append(m.find('a')['href'])\n",
    "    return movies_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = get_movies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ниже скачивает отзывы про один фильм и записывает их в словарь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_reviews(page_number):\n",
    "    url = f'https://afisha.ru{page_number}reviews/positive'\n",
    "    for _ in range(3): #update number of pages if necessary\n",
    "        response = session.get(url)\n",
    "        time.sleep(random.randint(3,10))\n",
    "        \n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    reviews = soup.find_all('div', class_ ='restrict-text review__text')\n",
    "    all_reviews = [review.text for review in reviews]\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fc02b32ea14be2bd532a3982459675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_pos = []\n",
    "\n",
    "for p in tqdm(movies):\n",
    "    pos_words = get_pos_reviews(p)\n",
    "    reviews_pos.extend(pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично с отрицательными отзывами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_reviews(page_number):\n",
    "    url = f'https://afisha.ru{page_number}reviews/negative'\n",
    "    for _ in range(3):\n",
    "        response = session.get(url)\n",
    "        time.sleep(random.randint(3,10))\n",
    "\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    reviews = soup.find_all('div', class_ ='restrict-text review__text')\n",
    "    all_reviews = [review.text for review in reviews]\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925b5402f1684b1ca75103752b989672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_neg = []\n",
    "\n",
    "for p in tqdm(movies):\n",
    "    neg_words = get_neg_reviews(p)\n",
    "    reviews_neg.extend(neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(review):\n",
    "    words = []\n",
    "    text = t.tokenize(review[1:].lower())\n",
    "    for word in text:\n",
    "        if word.isalpha():\n",
    "            lemmas = m.parse(word)[0].normal_form\n",
    "            words.append(lemmas)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(pos, neg):\n",
    "    pos_train, pos_test = train_test_split(pos)\n",
    "    neg_train, neg_test = train_test_split(neg)\n",
    "    \n",
    "    test_data = {}\n",
    "    \n",
    "    for word in pos_test:\n",
    "        test_data[word] = 1\n",
    "    for word in neg_test:\n",
    "        test_data[word] = 0\n",
    "    return pos_train, neg_train, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train, neg_train, test_data = split_data(reviews_pos, reviews_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classyfing reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция принимает лемматизированные положительные и отрицательные отзывы и помещает их в два отдельных списка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(pos, neg, freq):\n",
    "    pos_reviews = []\n",
    "    neg_reviews = []\n",
    "    for word in pos:\n",
    "        pos_reviews.extend(lemmatize(word))\n",
    "    for word in neg:\n",
    "        neg_reviews.extend(lemmatize(word))\n",
    "    \n",
    "    pos_freq = Counter(pos_reviews)\n",
    "    neg_freq = Counter(neg_reviews)\n",
    "    \n",
    "    pos_reviews = set(pos_reviews)\n",
    "    neg_reviews = set(neg_reviews)\n",
    "    \n",
    "    common = pos_reviews.intersection(neg_reviews)\n",
    "    \n",
    "    pos_reviews.symmetric_difference_update(common)\n",
    "    neg_reviews.symmetric_difference_update(common)\n",
    "    \n",
    "    pos_reviews = [word for word in pos_reviews if pos_freq[word] > freq]\n",
    "    neg_reviews = [word for word in neg_reviews if neg_freq[word] > freq]\n",
    "    return pos_reviews, neg_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive, negative = classify_review(pos_train, neg_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(text, pos, neg):    \n",
    "    words = lemmatize(text)\n",
    "    \n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in pos:\n",
    "            pos_score += 1\n",
    "        elif word in neg:\n",
    "            neg_score += 1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return pos_score, neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Positive.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 1 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Negative.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Negative.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Negative.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Negative.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Positive.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n",
      "Not clear.\n",
      "Answer: 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in test_data.keys(): # just to see whether everyting is working or not\n",
    "    pos_score, neg_score = prediction(text, positive, negative)\n",
    "    if pos_score > neg_score:\n",
    "        print('Positive.')\n",
    "    elif neg_score > pos_score:\n",
    "        print('Negative.')\n",
    "    else:\n",
    "        print('Not clear.')\n",
    "    print('Answer:', test_data[text], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5613\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "gold = []\n",
    "\n",
    "for text in test_data.keys():\n",
    "    pos_score, neg_score = prediction(text, positive, negative)\n",
    "    if pos_score > neg_score:\n",
    "        predicted.append(1)\n",
    "    else:\n",
    "        predicted.append(0)\n",
    "\n",
    "    if test_data[text] == 1:\n",
    "        gold.append(1)\n",
    "    else:\n",
    "        gold.append(0)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy_score(predicted, gold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самое простое и очевидное решение: увеличить количество данных. </p>\n",
    "\n",
    "Кроме того, было бы неплохо сбалансировать выборку (положительных отзывов получилось в два раза больше, чем отрицательных). Ещё можно использовать векторы с tf-idf вместо тональных словарей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2 (продолжение)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Лучший результат показал PyMoprhy, но Natasha удобнее в использовании, поэтому буду использовать именно её. <p>\n",
    "\n",
    "<p> Возьмём следующие биграммы и триграммы:  <p>\n",
    "\n",
    "1. \"Не\" + прилагательное (добавление частицы меняет положительную оценку на отрицательную.)\n",
    "2. \"Не\" + наречие (\"не плохо\")\n",
    "4. Прилагательное + существительное (стандартное описание понятий, \"увлекательный сюжет\", \"замечательная игра\")\n",
    "5. Наречие + глагол (отзывы типа \"плохо играют\")\n",
    "6. \"Не\" + наречие + глагол (\"не очень играют\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import(\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "doc = Doc(rus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, выделяющая n-граммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(text):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    \n",
    "    n_grams = []\n",
    "    for i in doc.tokens:\n",
    "        i.lemmatize(morph_vocab)\n",
    "        pos_tags = [(i.lemma, i.pos) for i in doc.tokens]\n",
    "        \n",
    "    for i in range(len(pos_tags)):\n",
    "        if (i+1) != len(pos_tags):\n",
    "            if pos_tags[i][0] == 'не':\n",
    "                if pos_tags[i+1][1] == 'ADJ':\n",
    "                    n_grams.append(' '.join([pos_tags[i][0], pos_tags[i+1][0]]))\n",
    "                elif pos_tags[i+1][1] == 'ADV':\n",
    "                    if pos_tags[i+2][1] == 'VERB':\n",
    "                        n_grams.append(' '.join([pos_tags[i][0], pos_tags[i+1][0], pos_tags[i+2][0]]))\n",
    "            elif pos_tags[i][1] == 'ADJ':\n",
    "                if pos_tags[i+1][1] == 'NOUN':\n",
    "                    n_grams.append(' '.join([pos_tags[i][0], pos_tags[i+1][0]]))\n",
    "            elif pos_tags[i][1] == 'ADV':\n",
    "                if pos_tags[i+1][1] == 'VERB':\n",
    "                    n_grams.append(' '.join([pos_tags[i][0], pos_tags[i+1][0]]))\n",
    "    \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Та же самая функция лемматизации, что и выше, просто перенесла сюда для удобства)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(review):\n",
    "    words = []\n",
    "    text = t.tokenize(review[1:].lower())\n",
    "    for word in text:\n",
    "        if word.isalpha():\n",
    "            lemmas = m.parse(word)[0].normal_form\n",
    "            words.append(lemmas)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review_ngrams(pos, neg, freq):\n",
    "    pos_reviews = []\n",
    "    neg_reviews = []\n",
    "    for word in pos:\n",
    "        pos_reviews.extend(lemmatizer(word))\n",
    "        pos_reviews.extend(find_ngrams(word)) # now with groups\n",
    "    for word in neg:\n",
    "        neg_reviews.extend(lemmatizer(word))\n",
    "        neg_reviews.extend(find_ngrams(word)) # same here\n",
    "    \n",
    "    pos_freq = Counter(pos_reviews)\n",
    "    neg_freq = Counter(neg_reviews)\n",
    "    \n",
    "    pos_reviews = set(pos_reviews)\n",
    "    neg_reviews = set(neg_reviews)\n",
    "    \n",
    "    common = pos_reviews.intersection(neg_reviews)\n",
    "    \n",
    "    pos_reviews.symmetric_difference_update(common)\n",
    "    neg_reviews.symmetric_difference_update(common)\n",
    "    \n",
    "    pos_reviews = [word for word in pos_reviews if pos_freq[word] > freq]\n",
    "    neg_reviews = [word for word in neg_reviews if neg_freq[word] > freq]\n",
    "    return pos_reviews, neg_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive, negative = classify_review_ngrams(pos_train, neg_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_ngrams(text, pos, neg):  \n",
    "    words = lemmatizer(text)\n",
    "    words.extend(find_ngrams(text)) # updated\n",
    "    \n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in pos:\n",
    "            pos_score += 1\n",
    "        elif word in neg:\n",
    "            neg_score += 1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return pos_score, neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "gold = []\n",
    "\n",
    "for text in test_data.keys():\n",
    "    pos_score, neg_score = prediction_ngrams(text, positive, negative) # updated\n",
    "    if pos_score > neg_score:\n",
    "        predicted.append(1)\n",
    "    else:\n",
    "        predicted.append(0)\n",
    "\n",
    "    if test_data[text] == 1:\n",
    "        gold.append(1)\n",
    "    else:\n",
    "        gold.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 105)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted), len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9238\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.4f' % accuracy_score(predicted, gold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5613 → 0.9238! Качество заметно улучшилось. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
